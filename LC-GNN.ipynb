{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class ModelNetDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', num_points=1024, single_view=True):\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.num_points = num_points\n",
    "        self.single_view = single_view\n",
    "        self.categories = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
    "        self.cat_to_idx = {cat: idx for idx, cat in enumerate(self.categories)}\n",
    "        self.data_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # 遍历所有类别文件夹\n",
    "        for cat in self.categories:\n",
    "            cat_dir = os.path.join(root_dir, cat, split)\n",
    "            if not os.path.exists(cat_dir):\n",
    "                continue\n",
    "            \n",
    "            # 获取该类别下所有.xyz文件\n",
    "            files = sorted([f for f in os.listdir(cat_dir) if f.endswith('.xyz')])\n",
    "            \n",
    "            if single_view:\n",
    "                # 只保留每个物体的第一个视角\n",
    "                unique_models = set('_'.join(f.split('_')[:-1]) for f in files)\n",
    "                files = [next(f for f in files if f.startswith(model)) for model in unique_models]\n",
    "            \n",
    "            cat_idx = self.cat_to_idx[cat]\n",
    "            self.data_paths.extend([os.path.join(cat_dir, f) for f in files])\n",
    "            self.labels.extend([cat_idx] * len(files))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        point_cloud = np.loadtxt(self.data_paths[idx]).astype(np.float32) \n",
    "        label = self.labels[idx]\n",
    "        point_cloud = torch.from_numpy(point_cloud) \n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        return point_cloud, label\n",
    "\n",
    "def get_data_loaders(root_dir='modelnetdata', batch_size=32, num_workers=None):\n",
    "    \n",
    "    if num_workers is None:\n",
    "        num_workers = 0 if os.name == 'nt' else 4  # Windows上设为0，Linux上设为4\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    pin_memory = device.type == 'cuda'  # 只在使用GPU时启用pin_memory\n",
    "    \n",
    "    train_dataset = ModelNetDataset(root_dir, split='train')\n",
    "    test_dataset = ModelNetDataset(root_dir, split='test')\n",
    "    \n",
    "    print(f\"训练集大小: {len(train_dataset)}\")\n",
    "    print(f\"测试集大小: {len(test_dataset)}\")\n",
    "    print(f\"类别数量: {len(train_dataset.categories)}\")\n",
    "    print(f\"使用设备: {device}\")\n",
    "    print(f\"数据加载进程数: {num_workers}\")\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader, device\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_loader, test_loader, device = get_data_loaders(batch_size=32)\n",
    "    for points, labels in train_loader:\n",
    "\n",
    "        points = points.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        print(f\"Batch points shape: {points.shape}\")\n",
    "        print(f\"Batch labels shape: {labels.shape}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for points, labels in train_loader:\n",
    "        points, labels = points.to(device), labels.to(device)\n",
    "        batch = torch.arange(points.size(0)).repeat_interleave(points.size(1)).to(device)\n",
    "        points = points.view(-1, 3)  # [B*N, 3]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = model(points, batch)\n",
    "        loss = criterion(out, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += pred.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for points, labels in test_loader:\n",
    "            points, labels = points.to(device), labels.to(device)\n",
    "            batch = torch.arange(points.size(0)).repeat_interleave(points.size(1)).to(device)\n",
    "            points = points.view(-1, 3)\n",
    "            \n",
    "            out = model(points, batch)\n",
    "            loss = criterion(out, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += pred.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    return total_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_scatter import scatter_max, scatter_mean, scatter_add\n",
    "from torch_geometric.nn import radius_graph\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_sparse import SparseTensor\n",
    "import torch_geometric.nn as gnn\n",
    "\n",
    "\n",
    "class EfficientLocalModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 结构重要性评分网络\n",
    "        self.importance_net = nn.Sequential(\n",
    "            nn.Linear(4, 32),  # 3(位置) + 1(加权局部度)\n",
    "            nn.LayerNorm(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def compute_weighted_local_degree(self, edge_index, pos, num_nodes):\n",
    "\n",
    "        row, col = edge_index\n",
    "        \n",
    "        # 计算边的距离权重\n",
    "        edge_dist = torch.norm(pos[row] - pos[col], dim=1)\n",
    "        dist_weight = torch.exp(-edge_dist)  # 距离越近权重越大\n",
    "        \n",
    "        # 计算基础度\n",
    "        degree = scatter_add(dist_weight, row, dim=0, dim_size=num_nodes)\n",
    "        \n",
    "        # 计算邻居的加权度\n",
    "        neighbor_degree = scatter_add(\n",
    "            degree[col] * dist_weight,  # 邻居的度 * 距离权重\n",
    "            row,\n",
    "            dim=0,\n",
    "            dim_size=num_nodes\n",
    "        )\n",
    "        \n",
    "        # 组合得分：结合自身度和邻居度\n",
    "        weighted_local_degree = degree + 0.5 * neighbor_degree / (degree + 1e-6)\n",
    "        \n",
    "        # 归一化\n",
    "        weighted_local_degree = weighted_local_degree / (weighted_local_degree.max() + 1e-6)\n",
    "        \n",
    "        return weighted_local_degree\n",
    "    \n",
    "    def forward(self, edge_index, pos, batch):\n",
    "        num_nodes = pos.size(0)\n",
    "        \n",
    "        # 计算加权局部度中心性\n",
    "        weighted_local_degree = self.compute_weighted_local_degree(edge_index, pos, num_nodes)\n",
    "        \n",
    "        # 组合特征\n",
    "        node_features = torch.cat([\n",
    "            pos,\n",
    "            weighted_local_degree.unsqueeze(1)\n",
    "        ], dim=1)\n",
    "        \n",
    "        # 计算结构重要性分数\n",
    "        importance_scores = self.importance_net(node_features)\n",
    "        \n",
    "        return importance_scores\n",
    "\n",
    "class StructuralEdgeConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, radius=0.1, max_num_neighbors=16):\n",
    "        super().__init__()\n",
    "        self.radius = radius\n",
    "        self.max_num_neighbors = max_num_neighbors\n",
    "        \n",
    "        # 边特征网络\n",
    "        self.edge_nn = nn.Sequential(\n",
    "            nn.Linear(in_channels * 2 + 1, out_channels),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.LeakyReLU(negative_slope=0.2)\n",
    "        )\n",
    "        \n",
    "        # 使用高效的局部结构模块\n",
    "        self.structural_module = EfficientLocalModule()\n",
    "        \n",
    "        # 特征转换\n",
    "        self.transform = nn.Sequential(\n",
    "            nn.Linear(out_channels, out_channels),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.LeakyReLU(negative_slope=0.2)\n",
    "        )\n",
    "        \n",
    "        # 自适应聚合权重\n",
    "        self.aggregate_weight = nn.Parameter(torch.ones(2))\n",
    "    \n",
    "    def forward(self, x, pos, batch):\n",
    "        # 构建球形邻域图\n",
    "        edge_index = radius_graph(\n",
    "            pos,\n",
    "            r=self.radius,\n",
    "            batch=batch,\n",
    "            max_num_neighbors=self.max_num_neighbors\n",
    "        )\n",
    "        \n",
    "        # 计算结构重要性分数\n",
    "        structural_scores = self.structural_module(edge_index, pos, batch)\n",
    "        \n",
    "        row, col = edge_index\n",
    "        edge_dist = torch.norm(pos[row] - pos[col], dim=-1)\n",
    "        \n",
    "        # 边特征学习\n",
    "        edge_feat = torch.cat([\n",
    "            x[row],\n",
    "            x[col] - x[row],\n",
    "            edge_dist.unsqueeze(-1)\n",
    "        ], dim=1)\n",
    "        edge_feat = self.edge_nn(edge_feat)\n",
    "        \n",
    "        # 结构重要性加权\n",
    "        edge_feat = edge_feat * structural_scores[row]\n",
    "        \n",
    "        # 自适应聚合\n",
    "        weights = F.softmax(self.aggregate_weight, dim=0)\n",
    "        out_max = scatter_max(edge_feat, row, dim=0, dim_size=x.size(0))[0]\n",
    "        out_mean = scatter_mean(edge_feat, row, dim=0, dim_size=x.size(0))\n",
    "        out = weights[0] * out_max + weights[1] * out_mean\n",
    "        \n",
    "        return self.transform(out)\n",
    "\n",
    "class StructuralGNN(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=40):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 特征提取层\n",
    "        self.feat_extract = nn.Sequential(\n",
    "            nn.Linear(in_channels, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # 多尺度结构感知卷积层\n",
    "        self.edge_conv1 = StructuralEdgeConv(64, 64, radius=0.1)\n",
    "        self.edge_conv2 = StructuralEdgeConv(64, 96, radius=0.2)\n",
    "        self.edge_conv3 = StructuralEdgeConv(96, 128, radius=0.4)\n",
    "        \n",
    "        # 残差连接\n",
    "        self.res1 = nn.Linear(64, 96)\n",
    "        self.res2 = nn.Linear(96, 128)\n",
    "        \n",
    "        # 特征稳定层\n",
    "        self.stabilize = nn.ModuleList([\n",
    "            nn.LayerNorm(64),\n",
    "            nn.LayerNorm(96),\n",
    "            nn.LayerNorm(128)\n",
    "        ])\n",
    "        \n",
    "        # 分类头\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(288, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, batch):\n",
    "        pos = x.clone()\n",
    "        \n",
    "        # 特征提取\n",
    "        x = self.feat_extract(x)\n",
    "        \n",
    "        # 多尺度特征提取\n",
    "        x1 = self.edge_conv1(x, pos, batch)\n",
    "        x1 = self.stabilize[0](x1)\n",
    "        \n",
    "        x2 = self.edge_conv2(x1, pos, batch)\n",
    "        x2 = self.stabilize[1](x2 + self.res1(x1))\n",
    "        \n",
    "        x3 = self.edge_conv3(x2, pos, batch)\n",
    "        x3 = self.stabilize[2](x3 + self.res2(x2))\n",
    "        \n",
    "        # 特征融合\n",
    "        x = torch.cat([x1, x2, x3], dim=1)\n",
    "        \n",
    "        # 全局池化\n",
    "        x = scatter_max(x, batch, dim=0)[0]\n",
    "        \n",
    "        # 分类\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch_cluster\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm  \n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def train(config):\n",
    "    \n",
    "    # 数据增强\n",
    "    def random_point_dropout(pos, max_dropout_ratio=0.2):\n",
    "        device = pos.device\n",
    "        batch_size, num_points, _ = pos.shape\n",
    "\n",
    "        dropout_ratio = torch.rand(batch_size, device=device) * max_dropout_ratio\n",
    "        point_masks = torch.rand(batch_size, num_points, device=device)\n",
    "\n",
    "        for i, ratio in enumerate(dropout_ratio):\n",
    "            point_masks[i] = (point_masks[i] <= (1 - ratio)).float()\n",
    "\n",
    "        pos = pos * point_masks.unsqueeze(-1)\n",
    "        return pos\n",
    "\n",
    "    def random_scale_point_cloud(pos, scale_low=0.8, scale_high=1.2):\n",
    "        device = pos.device\n",
    "        batch_size = pos.shape[0]\n",
    "        scales = torch.rand(batch_size, 1, 1, device=device) * (scale_high - scale_low) + scale_low\n",
    "        pos = pos * scales\n",
    "        return pos\n",
    "        \n",
    "    save_dir = os.path.join('runs', datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    train_loader, test_loader, device = get_data_loaders(\n",
    "        root_dir=config['data_dir'],\n",
    "        batch_size=config['batch_size'],\n",
    "        num_workers=config['num_workers']\n",
    "    )\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = StructuralGNN(  \n",
    "        in_channels=3,\n",
    "        num_classes=40\n",
    "    ).to(device)\n",
    "    \n",
    "    # 打印模型信息\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Model Parameters: {num_params/1e6:.2f}M\")\n",
    "    \n",
    "    # 优化器设置\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['lr'],\n",
    "        weight_decay=config['weight_decay'],\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "\n",
    "    # 学习率调度\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=config['lr'],\n",
    "        epochs=config['epochs'],\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.15,        # 预热阶段\n",
    "        div_factor=10,         # 初始学习率 = max_lr/10\n",
    "        final_div_factor=1e4,  # 最终学习率 = max_lr/10000\n",
    "        anneal_strategy='cos'  # 余弦退火\n",
    "    )\n",
    "    \n",
    "    warmup_epochs = 5\n",
    "    warmup_scheduler = optim.lr_scheduler.LinearLR(\n",
    "        optimizer, \n",
    "        start_factor=0.01,\n",
    "        end_factor=1.0,\n",
    "        total_iters=warmup_epochs * len(train_loader)\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    \n",
    "    # 训练记录\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'test_loss': [], 'test_acc': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    # 早停参数\n",
    "    best_acc = 0.0\n",
    "    patience = 25\n",
    "    no_improve = 0\n",
    "    \n",
    "    print(f\"\\nTraining Configuration:\")\n",
    "    print(f\"Epochs: {config['epochs']}\")\n",
    "    print(f\"Batch Size: {config['batch_size']}\")\n",
    "    print(f\"Learning Rate: {config['lr']}\")\n",
    "    print(f\"Weight Decay: {config['weight_decay']}\")\n",
    "    print(f\"Device: {device}\\n\")\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        \n",
    "        if epoch < warmup_epochs:\n",
    "            current_scheduler = warmup_scheduler\n",
    "        else:\n",
    "            current_scheduler = scheduler\n",
    "            \n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (points, labels) in enumerate(train_loader):\n",
    "            # 数据准备\n",
    "            points = random_point_dropout(points)\n",
    "            points = random_scale_point_cloud(points)\n",
    "            points = points.to(device)\n",
    "            labels = labels.to(device)\n",
    "            batch = torch.arange(points.size(0)).repeat_interleave(points.size(1)).to(device)\n",
    "            points = points.view(-1, 3)\n",
    "            \n",
    "            # 前向传播\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(points, batch)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            current_scheduler.step()\n",
    "            \n",
    "            # 统计\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # 打印进度\n",
    "            if (batch_idx + 1) % 50 == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                print(f'Epoch [{epoch+1}/{config[\"epochs\"]}] '\n",
    "                      f'Batch [{batch_idx+1}/{len(train_loader)}] '\n",
    "                      f'Loss: {train_loss/(batch_idx+1):.4f} '\n",
    "                      f'Acc: {100.*correct/total:.2f}% '\n",
    "                      f'LR: {scheduler.get_last_lr()[0]:.6f}')\n",
    "        \n",
    "        # 计算训练指标\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        # 测试\n",
    "        test_loss, test_acc = test(model, test_loader, criterion, device)\n",
    "        \n",
    "        # 更新历史记录\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        history['lr'].append(scheduler.get_last_lr()[0])\n",
    "        \n",
    "        print(f'\\nEpoch {epoch+1} Summary:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        \n",
    "        # 模型保存\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            no_improve = 0\n",
    "            print(f'Saving best model with accuracy: {best_acc:.4f}')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_acc': best_acc,\n",
    "                'config': config,\n",
    "            }, os.path.join(save_dir, 'best_model.pth'))\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f'\\nEarly stopping at epoch {epoch+1} after {patience} epochs without improvement')\n",
    "                break\n",
    "    \n",
    "    print(f'\\nTraining completed! Best test accuracy: {best_acc:.4f}')\n",
    "    return history, best_acc\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "config = {\n",
    "    'data_dir': 'modelnetdata',\n",
    "    'batch_size': 32,\n",
    "    'num_workers': 4,\n",
    "    'epochs': 300,  # 增加训练轮数\n",
    "    'lr': 0.001,    # 提高基础学习率\n",
    "    'weight_decay': 5e-4  # 增加权重衰减\n",
    "}\n",
    "\n",
    "set_seed()\n",
    "history, best_acc = train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制训练曲线\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# 损失曲线\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['test_loss'], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Testing Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 准确率曲线\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_acc'], label='Train Acc')\n",
    "plt.plot(history['test_acc'], label='Test Acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Testing Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def evaluate_model(model_path, test_loader, device):\n",
    "    \"\"\"完整评估模型在测试集上的性能\"\"\"\n",
    "    # 加载最佳模型\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model = StructuralGNN(in_channels=3, num_classes=40).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # 设置评估模式\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # 初始化指标\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(\"开始在完整测试集上评估...\")\n",
    "    with torch.no_grad():\n",
    "        for points, labels in test_loader:\n",
    "            points, labels = points.to(device), labels.to(device)\n",
    "            batch = torch.arange(points.size(0)).repeat_interleave(points.size(1)).to(device)\n",
    "            points = points.view(-1, 3)\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = model(points, batch)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # 统计\n",
    "            total_loss += loss.item()\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += pred.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            # 保存预测结果\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # 计算整体指标\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    print(\"\\n测试集评估结果:\")\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Correct Predictions: {correct}/{total}\")\n",
    "    \n",
    "    # 计算每类的准确率\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    for class_idx in range(40):\n",
    "        class_mask = (all_labels == class_idx)\n",
    "        if np.sum(class_mask) > 0:\n",
    "            class_acc = np.mean(all_preds[class_mask] == all_labels[class_mask])\n",
    "            print(f\"Class {class_idx} Accuracy: {class_acc:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels\n",
    "    }\n",
    "\n",
    "# 运行评估\n",
    "if __name__ == \"__main__\":\n",
    "    # 获取最新的模型文件\n",
    "    runs_dir = 'runs'\n",
    "    latest_run = max([os.path.join(runs_dir, d) for d in os.listdir(runs_dir)], key=os.path.getmtime)\n",
    "    model_path = \"/home/featurize/work/GNN Pointcloud/runs/20250103_114612/best_model.pth\"\n",
    "    \n",
    "    # 加载测试数据\n",
    "    _, test_loader, device = get_data_loaders(\n",
    "        root_dir='modelnetdata',\n",
    "        batch_size=32,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    # 评估模型\n",
    "    results = evaluate_model(model_path, test_loader, device)\n",
    "    \n",
    "    # 绘制混淆矩阵\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    cm = confusion_matrix(results['labels'], results['predictions'])\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class DynamicGraphVisualizer:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.activation = {}\n",
    "        \n",
    "        # 获取中间层输出\n",
    "        def get_activation(name):\n",
    "            def hook(model, input, output):\n",
    "                self.activation[name] = output\n",
    "            return hook\n",
    "            \n",
    "        self.model.edge_conv1.structural_module.register_forward_hook(get_activation('layer1_importance'))\n",
    "        self.model.edge_conv2.structural_module.register_forward_hook(get_activation('layer2_importance'))\n",
    "        self.model.edge_conv3.structural_module.register_forward_hook(get_activation('layer3_importance'))\n",
    "\n",
    "    def create_graph_figure(self, points, edge_index, centrality, title):\n",
    "\n",
    "        points_np = points.cpu().numpy()\n",
    "        edge_index_np = edge_index.cpu().numpy()\n",
    "        centrality_np = centrality.cpu().numpy()\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # 添加点云\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=points_np[:, 0],\n",
    "            y=points_np[:, 1],\n",
    "            z=points_np[:, 2],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=2,  \n",
    "                color=centrality_np,\n",
    "                colorscale='Viridis',\n",
    "                opacity=0.7 \n",
    "            ),\n",
    "            name='Points'\n",
    "        ))\n",
    "\n",
    "        # 随机采样边\n",
    "        num_edges = edge_index_np.shape[1]\n",
    "        sample_size = min(800, num_edges)  # 减少边的数量\n",
    "        sample_idx = np.random.choice(num_edges, sample_size, replace=False)\n",
    "\n",
    "        start_points = points_np[edge_index_np[0][sample_idx]]\n",
    "        end_points = points_np[edge_index_np[1][sample_idx]]\n",
    "\n",
    "        x_lines = []\n",
    "        y_lines = []\n",
    "        z_lines = []\n",
    "\n",
    "        for start, end in zip(start_points, end_points):\n",
    "            x_lines.extend([start[0], end[0], None])\n",
    "            y_lines.extend([start[1], end[1], None])\n",
    "            z_lines.extend([start[2], end[2], None])\n",
    "\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=x_lines,\n",
    "            y=y_lines,\n",
    "            z=z_lines,\n",
    "            mode='lines',\n",
    "            line=dict(\n",
    "                color='rgba(0,0,255,0.6)', \n",
    "                width=4 \n",
    "            ),\n",
    "            name='Edges'\n",
    "        ))\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=dict(text=title, x=0.5),\n",
    "            scene=dict(\n",
    "                xaxis_title='X',\n",
    "                yaxis_title='Y',\n",
    "                zaxis_title='Z',\n",
    "                aspectmode='data'\n",
    "            ),\n",
    "            showlegend=True\n",
    "        )\n",
    "\n",
    "        return fig\n",
    "    \n",
    "    def visualize_sample(self, points, batch, class_id, sample_idx, save_dir):\n",
    "        \"\"\"可视化单个样本的每一层图结构\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pos = points.clone()\n",
    "            x = self.model.feat_extract(points)\n",
    "            \n",
    "            # 第一层\n",
    "            edge_index1 = radius_graph(\n",
    "                pos, \n",
    "                r=self.model.edge_conv1.radius,\n",
    "                batch=batch,\n",
    "                max_num_neighbors=self.model.edge_conv1.max_num_neighbors\n",
    "            )\n",
    "            _ = self.model.edge_conv1(x, pos, batch)\n",
    "            importance1 = self.activation['layer1_importance']\n",
    "            \n",
    "            fig1 = self.create_graph_figure(\n",
    "                points,\n",
    "                edge_index1,\n",
    "                importance1,\n",
    "                f'Class {class_id} - Sample {sample_idx} - Layer 1'\n",
    "            )\n",
    "            fig1.write_html(os.path.join(save_dir, f'layer1_graph.html'))\n",
    "            \n",
    "            # 第二层\n",
    "            x1 = self.model.edge_conv1(x, pos, batch)\n",
    "            edge_index2 = radius_graph(\n",
    "                pos,\n",
    "                r=self.model.edge_conv2.radius, \n",
    "                batch=batch,\n",
    "                max_num_neighbors=self.model.edge_conv2.max_num_neighbors\n",
    "            )\n",
    "            _ = self.model.edge_conv2(x1, pos, batch)\n",
    "            importance2 = self.activation['layer2_importance']\n",
    "            \n",
    "            fig2 = self.create_graph_figure(\n",
    "                points,\n",
    "                edge_index2,\n",
    "                importance2, \n",
    "                f'Class {class_id} - Sample {sample_idx} - Layer 2'\n",
    "            )\n",
    "            fig2.write_html(os.path.join(save_dir, f'layer2_graph.html'))\n",
    "            \n",
    "            # 第三层 \n",
    "            x2 = self.model.edge_conv2(x1, pos, batch)\n",
    "            edge_index3 = radius_graph(\n",
    "                pos,\n",
    "                r=self.model.edge_conv3.radius,\n",
    "                batch=batch,\n",
    "                max_num_neighbors=self.model.edge_conv3.max_num_neighbors\n",
    "            )\n",
    "            _ = self.model.edge_conv3(x2, pos, batch)\n",
    "            importance3 = self.activation['layer3_importance']\n",
    "            \n",
    "            fig3 = self.create_graph_figure(\n",
    "                points,\n",
    "                edge_index3,\n",
    "                importance3,\n",
    "                f'Class {class_id} - Sample {sample_idx} - Layer 3'\n",
    "            )\n",
    "            fig3.write_html(os.path.join(save_dir, f'layer3_graph.html'))\n",
    "            \n",
    "def visualize_random_samples(test_loader, model, device, num_samples=3):\n",
    "\n",
    "    visualizer = DynamicGraphVisualizer(model, device)\n",
    "\n",
    "    save_dir = \"visualization_results\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # 获取一个batch的数据\n",
    "    for batch_data in test_loader:\n",
    "        points, labels = batch_data\n",
    "        batch_size = points.size(0)\n",
    "\n",
    "        # 随机选择样本\n",
    "        selected_indices = torch.randperm(batch_size)[:num_samples]\n",
    "\n",
    "        for idx in selected_indices:\n",
    "            sample_points = points[idx].to(device)  # [N, 3]\n",
    "            sample_label = labels[idx].item()\n",
    "\n",
    "            sample_batch = torch.zeros(sample_points.size(0), \n",
    "                                    dtype=torch.long, \n",
    "                                    device=device)\n",
    "\n",
    "            sample_dir = os.path.join(save_dir, f'sample_{idx}_class_{sample_label}')\n",
    "            os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "            visualizer.visualize_sample(\n",
    "                sample_points,\n",
    "                sample_batch,\n",
    "                sample_label,\n",
    "                idx,\n",
    "                sample_dir\n",
    "            )\n",
    "\n",
    "        break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(model_path)\n",
    "model = StructuralGNN(in_channels=3, num_classes=40).to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# 可视化随机样本\n",
    "visualize_random_samples(test_loader, model, device, num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import json\n",
    "\n",
    "class PointCloudVisualizer:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.activation = {}\n",
    "        \n",
    "        # 注册获取中间层输出\n",
    "        def get_activation(name):\n",
    "            def hook(model, input, output):\n",
    "                self.activation[name] = output\n",
    "            return hook\n",
    "            \n",
    "        self.model.edge_conv1.structural_module.register_forward_hook(get_activation('layer1_importance'))\n",
    "        self.model.edge_conv2.structural_module.register_forward_hook(get_activation('layer2_importance'))\n",
    "        self.model.edge_conv3.structural_module.register_forward_hook(get_activation('layer3_importance'))\n",
    "\n",
    "    def visualize_connections(self, points, batch, class_id, sample_idx, save_dir):\n",
    "        \"\"\"可视化单个样本中中心性最大点的多层连接\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pos = points.clone()\n",
    "            x = self.model.feat_extract(points)\n",
    "            \n",
    "            # 获取第一层的结构重要性分数\n",
    "            _ = self.model.edge_conv1(x, pos, batch)\n",
    "            importance1 = self.activation['layer1_importance'].squeeze()\n",
    "            \n",
    "            # 选择重要性分数最大的点作为中心点\n",
    "            center_idx = importance1.argmax().item()\n",
    "            \n",
    "            # 获取三层的连接\n",
    "            edge_index1 = radius_graph(\n",
    "                pos, \n",
    "                r=self.model.edge_conv1.radius,\n",
    "                batch=batch,\n",
    "                max_num_neighbors=self.model.edge_conv1.max_num_neighbors\n",
    "            )\n",
    "            \n",
    "            # 计算第二层\n",
    "            x1 = self.model.edge_conv1(x, pos, batch)\n",
    "            edge_index2 = radius_graph(\n",
    "                pos,\n",
    "                r=self.model.edge_conv2.radius,\n",
    "                batch=batch,\n",
    "                max_num_neighbors=self.model.edge_conv2.max_num_neighbors\n",
    "            )\n",
    "            \n",
    "            # 计算第三层\n",
    "            x2 = self.model.edge_conv2(x1, pos, batch)\n",
    "            edge_index3 = radius_graph(\n",
    "                pos,\n",
    "                r=self.model.edge_conv3.radius,\n",
    "                batch=batch,\n",
    "                max_num_neighbors=self.model.edge_conv3.max_num_neighbors\n",
    "            )\n",
    "            \n",
    "            # 获取与中心点相连的点索引\n",
    "            neighbors1 = edge_index1[1][edge_index1[0] == center_idx].cpu().numpy()\n",
    "            neighbors2 = edge_index2[1][edge_index2[0] == center_idx].cpu().numpy()\n",
    "            neighbors3 = edge_index3[1][edge_index3[0] == center_idx].cpu().numpy()\n",
    "            \n",
    "            colors = np.ones((points.size(0), 3)) * 0.8 \n",
    "            \n",
    "            # 第三层连接点 - 最浅色\n",
    "            colors[neighbors3] = np.array([0.2, 0.6, 1.0]) \n",
    "            \n",
    "            # 第二层连接点 - 中等深度\n",
    "            colors[neighbors2] = np.array([0.1, 0.4, 0.8])  \n",
    "            \n",
    "            # 第一层连接点 - 最深色\n",
    "            colors[neighbors1] = np.array([0.0, 0.2, 0.6])  \n",
    "            \n",
    "            # 中心点标红\n",
    "            colors[center_idx] = np.array([1.0, 0.0, 0.0]) \n",
    "            \n",
    "            # 创建3D散点图\n",
    "            fig = plt.figure(figsize=(10, 10))\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "            \n",
    "            points_np = points.cpu().numpy()\n",
    "            ax.scatter(points_np[:, 0], points_np[:, 1], points_np[:, 2], \n",
    "                      c=colors, s=20)\n",
    "            \n",
    "            ax.set_facecolor('white')\n",
    "            fig.patch.set_facecolor('white')\n",
    "            \n",
    "            importance_score = importance1[center_idx].item()\n",
    "            plt.title(f'Class {class_id} - Sample {sample_idx}\\n' + \n",
    "                     f'Center Point Importance: {importance_score:.3f}\\n' +\n",
    "                     f'Connections: Layer1={len(neighbors1)}, ' +\n",
    "                     f'Layer2={len(neighbors2)}, Layer3={len(neighbors3)}')\n",
    "            \n",
    "            plt.savefig(os.path.join(save_dir, \n",
    "                       f'connections_class{class_id}_sample{sample_idx}_imp{importance_score:.3f}.png'),\n",
    "                       bbox_inches='tight', dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "            connection_info = {\n",
    "                'class_id': class_id,\n",
    "                'sample_idx': sample_idx,\n",
    "                'center_point_idx': center_idx,\n",
    "                'importance_score': importance_score,\n",
    "                'num_connections': {\n",
    "                    'layer1': len(neighbors1),\n",
    "                    'layer2': len(neighbors2),\n",
    "                    'layer3': len(neighbors3)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            with open(os.path.join(save_dir, \n",
    "                     f'info_class{class_id}_sample{sample_idx}.json'), 'w') as f:\n",
    "                json.dump(connection_info, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_random_samples_from_different_classes(test_loader, model, device, num_samples=3):\n",
    "    visualizer = PointCloudVisualizer(model, device)\n",
    "    save_dir = \"visualization_results_connections\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    for batch_data in test_loader:\n",
    "        points, labels = batch_data\n",
    "        all_data.extend(points)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    class_data = {}\n",
    "    for data, label in zip(all_data, all_labels):\n",
    "        label = label.item()\n",
    "        if label not in class_data:\n",
    "            class_data[label] = []\n",
    "        class_data[label].append(data)\n",
    "    \n",
    "    # 随机选择不同的类别\n",
    "    selected_classes = random.sample(list(class_data.keys()), num_samples)\n",
    "    \n",
    "    for i, class_id in enumerate(selected_classes):\n",
    "        # 从该类别中随机选择一个样本\n",
    "        sample_data = random.choice(class_data[class_id])\n",
    "        sample_data = sample_data.to(device)\n",
    "        \n",
    "        # 创建batch向量\n",
    "        sample_batch = torch.zeros(sample_data.size(0), dtype=torch.long, device=device)\n",
    "        \n",
    "        visualizer.visualize_connections(\n",
    "            sample_data,\n",
    "            sample_batch,\n",
    "            class_id,\n",
    "            i,\n",
    "            save_dir\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(model_path)\n",
    "model = StructuralGNN(in_channels=3, num_classes=40).to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "    \n",
    "# 可视化来自不同类别的随机样本\n",
    "visualize_random_samples_from_different_classes(test_loader, model, device, num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
